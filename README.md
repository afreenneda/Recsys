## Explainable Recommendation through Knowledge Graphs in Education

## Requirements
- Python 3.8
- Install the required packages:
```sh
pip install -r requirements.txt 
```

Download the datasets from the Google drive: https://drive.google.com/drive/folders/1MAxH1HbowFU7uJeegtVbMZgVTIuVAmMq?usp=drive_link
Then extract **data.zip** inside the **top level** of the repository (i.e. the level in which setup.py is located). 

The base workflow is:

### 0. Install the repository
From the top-level (i.e. the folder which contains setup.py and the pathlm folder)
Run:
```sh
pip install .
```
### 1.Models
- DATASET_NAME {coco, mooper, mooccube} MODEL_NAMES:
- traditional methods{nfm, bprmf}
- knowledge aware methods{cke, kgat}
- path based methods{pgpr, cafe}
- casual language models{plm, pearlm}

### 2. Dataset generation
To create the `preprocessed/mapping` folder needed by the algorithm, run from the top level:
```sh
python pathlm/data_mappers/map_dataset.py --data <dataset_name> --model <model_name>
```

Note: For Casual Language Model, each dataset is generated by the pipeline described in 'create_dataset.sh' which is in charge of:
1. Generation of a dataset of **at most X unique paths per user**
2. Concatenation of the results into a single .txt file
3. (Optional) Pruning of the concatenated .txt file (This is only useful if the start entity is chosen instead of the standard 'USER')
4. Move of the concatenated and pruned .txt file into the 'data' folder which is used to tokenize and train the models

### 3. Bulk Training
From the top-level (i.e. the folder which contains setup.py and the pathlm folder).
Install the repository with ```pip install .```

Then, proceed according to the chosen experiment to run as described below.
Each bash script can be customised as desired in order to run alternative experiments

##### TransE Embeddings
Methods such as (PGPR, CAFE, PLM, PEARLM) have dependencies with the TransE Embeddings, run mapper to map in standard format and transe model for TransE representation
```sh
python pathlm/data_mappers/map_dataset.py --data <dataset_name> --model transe
```
```sh
python pathlm/models/embeddings/train_transe_model.py --dataset <dataset_name>
```

##### Run path based method{PGPR, CAFE}
Before training, preprocessed data for the respective model, run from the top level:
#### PGPR
```sh
python pathlm/models/rl/PGPR/preprocess.py --dataset <dataset_name>
```
train agent by executing:
```sh
python pathlm/models/rl/PGPR/train_agent.py --dataset <dataset_name>
```
#### CAFE
```sh
python pathlm/models/rl/CAFE/preprocess.py --dataset <dataset_name>
```
learn the policy by executing:
```sh
 python pathlm/models/rl/CAFE/train_neural_symbol.py --dataset <dataset_name>
```

##### Casual Language Modeling for Path Reasoning
### Path Sampling
Sampling can be employed by running 
```sh
 create_dataset.sh
```
script and specify the positional parameters:
1. dataset: {coco, mooper, mooccube}
2. sample_size: represent the amount of paths sampled for each user
3. n_hop: represent the fixed hop size for the paths sampled
4. n_proc: number of processors to employ for multiprocessing operations

```sh
bash create_dataset.sh {dataset_name} {sample_size} {n_hop} {n_proc}
```

### Tokenizing
Before training a PLM or PEARLM, tokenize the dataset, running from the top level:
```sh
python pathlm/models/lm/tokenize_dataset.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}
```

##### PLM-Rec
To train PLM-Rec, run from the top level:
```sh
python pathlm/models/lm/plm_main.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}
```


##### PERLM
To train a specific PEARLM, run from the top level:
```
python pathlm/models/lm/pearlm_main.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}
```

